{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted version of \n",
    "# https://www.kaggle.com/cbrogan/titanic/xgboost-example-python/code\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_df = pd.read_csv('../data/train.csv', header=0)\n",
    "test_df = pd.read_csv('../data/test.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['Intact'], train_df['Sex'] = train_df\\\n",
    "    .SexuponOutcome.str.split(' ').str\n",
    "train_df['Intact'] = train_df['Intact']\\\n",
    "    .replace('Unknown', np.nan)\n",
    "train_df['Intact'] = train_df['Intact']\\\n",
    "    .replace('Spayed', 'Neutered')\n",
    "\n",
    "test_df['Intact'], test_df['Sex'] = test_df\\\n",
    "    .SexuponOutcome.str.split(' ').str\n",
    "test_df['Intact'] = test_df['Intact']\\\n",
    "    .replace('Unknown', np.nan)\n",
    "\n",
    "def calc_age_in_days(df):\n",
    "    factor = {'year': 365, 'month': 31, 'week': 7, 'day': 1}\n",
    "    result = []\n",
    "    for age in df.AgeuponOutcome:\n",
    "        if str(age) != 'nan':\n",
    "            value, unit = age.split(' ')\n",
    "            days = \\\n",
    "                int(value) * factor[unit.replace('s', '')] \n",
    "            # ignore year[s], month[s], ...\n",
    "            result.append(days)\n",
    "        else:\n",
    "            result.append(np.nan)\n",
    "    df['AgeuponOutcomeInDays'] = result\n",
    "    return df\n",
    "\n",
    "def calc_age_in_years(df):\n",
    "    result = []\n",
    "    for age in df.AgeuponOutcomeInDays:\n",
    "        if str(age) != 'nan':\n",
    "            years = int(age / 365)\n",
    "            result.append(years)\n",
    "        else:\n",
    "            result.append(np.nan)\n",
    "    df['AgeuponOutcomeInYears'] = result\n",
    "    return df\n",
    "\n",
    "train_df = calc_age_in_days(train_df)\n",
    "train_df = calc_age_in_years(train_df)\n",
    "test_df = calc_age_in_days(test_df)\n",
    "test_df = calc_age_in_years(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split `SexuponOutcome` into `Intact` and `Sex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "      <th>AgeuponOutcomeInDays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Female</td>\n",
       "      <td>Cairn Terrier/Chihuahua Shorthair</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cat</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>American Pit Bull Terrier Mix</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>Cairn Terrier</td>\n",
       "      <td>365.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AnimalType SexuponOutcome                              Breed  \\\n",
       "5        Dog  Intact Female  Cairn Terrier/Chihuahua Shorthair   \n",
       "6        Cat    Intact Male             Domestic Shorthair Mix   \n",
       "7        Cat        Unknown             Domestic Shorthair Mix   \n",
       "8        Dog  Spayed Female      American Pit Bull Terrier Mix   \n",
       "9        Dog  Spayed Female                      Cairn Terrier   \n",
       "\n",
       "   AgeuponOutcomeInDays  \n",
       "5                  31.0  \n",
       "6                  21.0  \n",
       "7                  21.0  \n",
       "8                 155.0  \n",
       "9                 365.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll impute missing values using the median for numeric\n",
    "# columns and the most common value for string columns.\n",
    "# This is based on some nice code by 'sveitser' at\n",
    "# http://stackoverflow.com/a/25562948\n",
    "from sklearn.base import TransformerMixin\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "                               if X[c].dtype == np.dtype('O')\n",
    "                               else X[c].median() for c in X],\n",
    "                              index=X.columns)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n",
    "feature_columns_to_use = ['AnimalType', 'SexuponOutcome', 'Breed',\n",
    "                          'AgeuponOutcomeInDays']  \n",
    "# <- best features so far\n",
    "\n",
    "nonnumeric_columns = feature_columns_to_use\n",
    "\n",
    "# Join the features from train and test together before \n",
    "# imputing missing values, in case their distribution is \n",
    "# slightly different\n",
    "# Do we really wanna do this? S/b said this is bad practice \n",
    "# for some reason\n",
    "big_X = train_df[feature_columns_to_use].append(\n",
    "    test_df[feature_columns_to_use])\n",
    "big_X_imputed = DataFrameImputer().fit_transform(big_X)\n",
    "big_X_imputed[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in missing fields:\n",
    "* `mean` if column is numeric\n",
    "* `most common value` if not numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See \n",
    "# http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\n",
    "# for more details and options\n",
    "le = LabelEncoder()\n",
    "for feature in nonnumeric_columns:\n",
    "    big_X_imputed[feature] = le.fit_transform(big_X_imputed[feature])\n",
    "\n",
    "# Prepare the inputs for the model\n",
    "train_X = big_X_imputed[0:train_df.shape[0]].as_matrix()\n",
    "test_X = big_X_imputed[train_df.shape[0]::].as_matrix()\n",
    "train_df.OutcomeType = le.fit_transform(train_df.OutcomeType)\n",
    "train_y = train_df['OutcomeType']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the actual learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_X, train_y, test_size=0.01, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_test_split` is a shorthand to randomly split a training set into train/test data for cross validation. \n",
    "\n",
    "* `test_size`: sets the percentage of data used for the test set.\n",
    "* `random_state`: is used to get reproducible randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cross validation training set\n",
    "xgmat = xgb.DMatrix(X_train, label=y_train)\n",
    "# cross validation test set\n",
    "xgmat_valid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "# the original test set from kaggle\n",
    "xgmat_test = xgb.DMatrix(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms our generated cross validation data to a format readable by `xgboost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Some information about parameter tweaking: [click](http://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
    "\n",
    "### objective\n",
    "Set against which loss function the algorithm should minimize. What we need: Multiclass with probabilities. So we choose `multi:softprob`. This in return needs a parameter `num_class` which indicates the number of differenct classes. In our case thats the number of distinct values for cloumn `OutcomeType`\n",
    "\n",
    "### Others\n",
    "* `eval_metric`: set to `mlogloss`, that is the one kaggle uses\n",
    "* `max_delta_setp`: set to number > 0 to fight imbalanced data sets (http://xgboost.readthedocs.io/en/latest/how_to/param_tuning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "    'objective': 'multi:softprob',\n",
    "    # should be 5\n",
    "    'num_class': train_df.OutcomeType.nunique(),\n",
    "    # feature weight shrinkrage: lower -> reduce complexity\n",
    "    'eta': 0.35,\n",
    "    # loss reduction needed to split a leaf and add another \n",
    "    # layer: higher -> reduces complexity\n",
    "    'gamma': 0.00,\n",
    "    # maximum depth of a tree: higher -> increase complexity\n",
    "    'max_depth': 8,\n",
    "    # minimum number of instances in a leaf: higher -> less complexity\n",
    "    # 'min_child_weight': 12,  \n",
    "    # in logregression a higher value might help with imbalanced classes\n",
    "    'max_delta_step': 1,\n",
    "    # randomly select instances from train set to \n",
    "    # build trees: lower -> less overfitting\n",
    "    'subsample': 0.5,\n",
    "    # subsample colums: lower -> less overfitting\n",
    "    'colsample_bytree': 0.5,\n",
    "    # regularization weight: higher -> less complex\n",
    "    'alpha': 0,\n",
    "    # fight unbalanced classes: sum(negative cases) / sum(positive cases)\n",
    "    'scale_pos_weight': 0,  \n",
    "    'eval_metric' : 'mlogloss',\n",
    "    'silent': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 100 rounds.\n",
      "[0]\ttrain-mlogloss:1.418455\teval-mlogloss:1.429267\n",
      "[1]\ttrain-mlogloss:1.246294\teval-mlogloss:1.262411\n",
      "[2]\ttrain-mlogloss:1.164008\teval-mlogloss:1.181612\n",
      "[3]\ttrain-mlogloss:1.094919\teval-mlogloss:1.116524\n",
      "[4]\ttrain-mlogloss:1.040706\teval-mlogloss:1.064858\n",
      "[5]\ttrain-mlogloss:1.002848\teval-mlogloss:1.032077\n",
      "[6]\ttrain-mlogloss:0.964524\teval-mlogloss:0.999383\n",
      "[7]\ttrain-mlogloss:0.941076\teval-mlogloss:0.981549\n",
      "[8]\ttrain-mlogloss:0.923167\teval-mlogloss:0.964063\n",
      "[9]\ttrain-mlogloss:0.906709\teval-mlogloss:0.949384\n",
      "[10]\ttrain-mlogloss:0.891419\teval-mlogloss:0.937201\n",
      "[11]\ttrain-mlogloss:0.878990\teval-mlogloss:0.925890\n",
      "[12]\ttrain-mlogloss:0.867721\teval-mlogloss:0.920318\n",
      "[13]\ttrain-mlogloss:0.862581\teval-mlogloss:0.916676\n",
      "[14]\ttrain-mlogloss:0.859031\teval-mlogloss:0.917146\n",
      "[15]\ttrain-mlogloss:0.851402\teval-mlogloss:0.912880\n",
      "[16]\ttrain-mlogloss:0.846427\teval-mlogloss:0.906657\n",
      "[17]\ttrain-mlogloss:0.841754\teval-mlogloss:0.901629\n",
      "[18]\ttrain-mlogloss:0.839387\teval-mlogloss:0.899473\n",
      "[19]\ttrain-mlogloss:0.836849\teval-mlogloss:0.896381\n",
      "[20]\ttrain-mlogloss:0.833613\teval-mlogloss:0.894829\n",
      "[21]\ttrain-mlogloss:0.831712\teval-mlogloss:0.892805\n",
      "[22]\ttrain-mlogloss:0.829559\teval-mlogloss:0.892414\n",
      "[23]\ttrain-mlogloss:0.827441\teval-mlogloss:0.888603\n",
      "[24]\ttrain-mlogloss:0.825994\teval-mlogloss:0.886548\n",
      "[25]\ttrain-mlogloss:0.824660\teval-mlogloss:0.886329\n",
      "[26]\ttrain-mlogloss:0.824259\teval-mlogloss:0.884934\n",
      "[27]\ttrain-mlogloss:0.823293\teval-mlogloss:0.886157\n",
      "[28]\ttrain-mlogloss:0.821284\teval-mlogloss:0.885735\n",
      "[29]\ttrain-mlogloss:0.820127\teval-mlogloss:0.886680\n",
      "[30]\ttrain-mlogloss:0.818551\teval-mlogloss:0.884123\n",
      "[31]\ttrain-mlogloss:0.816926\teval-mlogloss:0.882980\n",
      "[32]\ttrain-mlogloss:0.815827\teval-mlogloss:0.883492\n",
      "[33]\ttrain-mlogloss:0.814193\teval-mlogloss:0.883853\n",
      "[34]\ttrain-mlogloss:0.813351\teval-mlogloss:0.882983\n",
      "[35]\ttrain-mlogloss:0.812753\teval-mlogloss:0.883774\n",
      "[36]\ttrain-mlogloss:0.811996\teval-mlogloss:0.882968\n",
      "[37]\ttrain-mlogloss:0.810216\teval-mlogloss:0.886195\n",
      "[38]\ttrain-mlogloss:0.808829\teval-mlogloss:0.885737\n",
      "[39]\ttrain-mlogloss:0.808486\teval-mlogloss:0.884862\n",
      "[40]\ttrain-mlogloss:0.807758\teval-mlogloss:0.885507\n",
      "[41]\ttrain-mlogloss:0.807257\teval-mlogloss:0.885163\n",
      "[42]\ttrain-mlogloss:0.806540\teval-mlogloss:0.885474\n",
      "[43]\ttrain-mlogloss:0.806173\teval-mlogloss:0.885837\n",
      "[44]\ttrain-mlogloss:0.805473\teval-mlogloss:0.884894\n",
      "[45]\ttrain-mlogloss:0.805010\teval-mlogloss:0.882119\n",
      "[46]\ttrain-mlogloss:0.803983\teval-mlogloss:0.884323\n",
      "[47]\ttrain-mlogloss:0.803560\teval-mlogloss:0.883797\n",
      "[48]\ttrain-mlogloss:0.802450\teval-mlogloss:0.885555\n",
      "[49]\ttrain-mlogloss:0.801145\teval-mlogloss:0.887577\n",
      "[50]\ttrain-mlogloss:0.799564\teval-mlogloss:0.887596\n",
      "[51]\ttrain-mlogloss:0.799311\teval-mlogloss:0.888241\n",
      "[52]\ttrain-mlogloss:0.798959\teval-mlogloss:0.888978\n",
      "[53]\ttrain-mlogloss:0.798115\teval-mlogloss:0.890452\n",
      "[54]\ttrain-mlogloss:0.797667\teval-mlogloss:0.890286\n",
      "[55]\ttrain-mlogloss:0.796981\teval-mlogloss:0.891094\n",
      "[56]\ttrain-mlogloss:0.796727\teval-mlogloss:0.891987\n",
      "[57]\ttrain-mlogloss:0.795564\teval-mlogloss:0.892532\n",
      "[58]\ttrain-mlogloss:0.795161\teval-mlogloss:0.891346\n",
      "[59]\ttrain-mlogloss:0.794997\teval-mlogloss:0.889290\n",
      "[60]\ttrain-mlogloss:0.794467\teval-mlogloss:0.890221\n",
      "[61]\ttrain-mlogloss:0.794084\teval-mlogloss:0.891668\n",
      "[62]\ttrain-mlogloss:0.792822\teval-mlogloss:0.887502\n",
      "[63]\ttrain-mlogloss:0.792484\teval-mlogloss:0.888070\n",
      "[64]\ttrain-mlogloss:0.792035\teval-mlogloss:0.890144\n",
      "[65]\ttrain-mlogloss:0.791526\teval-mlogloss:0.890794\n",
      "[66]\ttrain-mlogloss:0.791449\teval-mlogloss:0.891577\n",
      "[67]\ttrain-mlogloss:0.790435\teval-mlogloss:0.890610\n",
      "[68]\ttrain-mlogloss:0.790114\teval-mlogloss:0.890207\n",
      "[69]\ttrain-mlogloss:0.789526\teval-mlogloss:0.891357\n",
      "[70]\ttrain-mlogloss:0.788454\teval-mlogloss:0.892213\n",
      "[71]\ttrain-mlogloss:0.787638\teval-mlogloss:0.893032\n",
      "[72]\ttrain-mlogloss:0.787231\teval-mlogloss:0.890354\n",
      "[73]\ttrain-mlogloss:0.787123\teval-mlogloss:0.888873\n",
      "[74]\ttrain-mlogloss:0.786222\teval-mlogloss:0.886916\n",
      "[75]\ttrain-mlogloss:0.786104\teval-mlogloss:0.885772\n",
      "[76]\ttrain-mlogloss:0.785913\teval-mlogloss:0.885179\n",
      "[77]\ttrain-mlogloss:0.785822\teval-mlogloss:0.886473\n",
      "[78]\ttrain-mlogloss:0.785535\teval-mlogloss:0.886731\n",
      "[79]\ttrain-mlogloss:0.784144\teval-mlogloss:0.887646\n",
      "[80]\ttrain-mlogloss:0.783747\teval-mlogloss:0.888351\n",
      "[81]\ttrain-mlogloss:0.783320\teval-mlogloss:0.888819\n",
      "[82]\ttrain-mlogloss:0.782662\teval-mlogloss:0.887509\n",
      "[83]\ttrain-mlogloss:0.782301\teval-mlogloss:0.888541\n",
      "[84]\ttrain-mlogloss:0.781046\teval-mlogloss:0.889863\n",
      "[85]\ttrain-mlogloss:0.780885\teval-mlogloss:0.890138\n",
      "[86]\ttrain-mlogloss:0.780707\teval-mlogloss:0.892116\n"
     ]
    }
   ],
   "source": [
    "num_round=3000\n",
    "watchlist = [(xgmat, 'train'), (xgmat_valid, 'eval')]\n",
    "gbm = xgb.train(param, xgmat, num_round, watchlist, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't just simply learn a model but we observe how the precision evolves [more info](http://xgboost.readthedocs.io/en/latest/python/python_intro.html#training)\n",
    "* `early_stopping_rounds`: will stop training if the precision did not improve in the set number of rounds\n",
    "* `watchlist`: contains the parameters to watch while learning\n",
    "\n",
    "__*train() only returns the last model, not necessarily the best one!*__  see [here](http://xgboost.readthedocs.io/en/latest/python/python_intro.html#prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needs graphviz: 'sudo apt-get install graphviz' and 'pip3 install graphviz'\n",
    "xgb.to_graphviz(gbm, num_trees=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and save as Kaggel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = gbm.predict(xgmat_test, ntree_limit=gbm.best_ntree_limit)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to kaggle format\n",
    "submission_df = pd.DataFrame(predictions)\n",
    "submission_df = pd.concat([test_df.ID, submission_df], axis=1)\n",
    "submission_df.ID = submission_df.ID.astype(int)\n",
    "submission_df.set_index('ID', inplace=True)\n",
    "# get the string labels back\n",
    "submission_df.columns = le.inverse_transform(\n",
    "    sorted(train_df.OutcomeType.unique()))\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv')\n",
    "len(submission_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
